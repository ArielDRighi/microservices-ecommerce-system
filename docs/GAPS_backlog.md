# üîç An√°lisis de Gaps del Backlog - Proyecto 3: Microservicios E-commerce

> **Documento:** Revisi√≥n exhaustiva del backlog actual  
> **Fecha:** 2025-10-16  
> **Proyecto:** Sistema de gesti√≥n de inventario y √≥rdenes con microservicios  
> **Autor:** An√°lisis de arquitectura y planificaci√≥n

---

## üìä Resumen Ejecutivo

**Estado Actual del Backlog:** 60% Completo

- ‚úÖ Estructura s√≥lida con fases y √©picas bien organizadas
- ‚ö†Ô∏è Faltan ~50 tareas cr√≠ticas para estar production-ready
- üî¥ 7 gaps cr√≠ticos que deben a√±adirse antes de comenzar Fase 2

**Veredicto:** El backlog actual NO es suficiente para desarrollar el proyecto completo sin encontrar bloqueos a mitad del camino.

---

## ‚úÖ Fortalezas del Backlog Actual

### Lo que est√° bien implementado:

1. ‚úÖ **Fases bien definidas** (1-7) con objetivos claros por semana
2. ‚úÖ **√âpicas agrupadas l√≥gicamente** por funcionalidad t√©cnica
3. ‚úÖ **Separaci√≥n total del Proyecto 2** documentada (puertos, BDs, contenedores)
4. ‚úÖ **TDD mencionado** desde el inicio del Inventory Service
5. ‚úÖ **Estructura hexagonal** definida claramente para Go
6. ‚úÖ **M√©tricas de √©xito** establecidas al inicio del documento
7. ‚úÖ **Docker Compose** planificado desde Fase 1
8. ‚úÖ **CI/CD** considerado temprano en el proyecto

---

## ‚ùå GAPS CR√çTICOS IDENTIFICADOS

### üî¥ Gap 1: Migraci√≥n y Adaptaci√≥n del Orders Service

**Problema:** El backlog asume que simplemente "clonas" el Proyecto 2, pero hay refactoring masivo necesario para que funcione en un ecosistema de microservicios.

#### Epic 1.6: Refactoring del Orders Service para Microservicios

**Priority:** CRITICAL | **Effort:** 12h | **Dependencies:** Epic 1.1, 1.2

##### ‚è≥ T1.6.1: Eliminar l√≥gica de inventario interno

- Remover tabla `inventory` de la base de datos del Orders Service
- Eliminar seeders relacionados con inventario
- Eliminar endpoints internos `/inventory/*`
- Actualizar migraciones para eliminar referencias

##### ‚è≥ T1.6.2: Crear InventoryServiceClient (HTTP)

- Crear interface `IInventoryClient` en m√≥dulo com√∫n
- Implementaci√≥n con axios/fetch para llamadas HTTP
- Manejo de errores de red (timeouts, 5xx, connection refused)
- Retry logic con exponential backoff (3 intentos, delay: 1s, 2s, 4s)
- Logging estructurado de todas las llamadas

##### ‚è≥ T1.6.3: Actualizar Saga Pattern para llamadas externas

- Modificar `OrderSaga` para usar `InventoryServiceClient`
- A√±adir step de compensaci√≥n para fallos de red
- Implementar timeout en llamadas al servicio externo (max 5 segundos)
- Manejar casos de servicio no disponible

##### ‚è≥ T1.6.4: Actualizar variables de entorno

- `INVENTORY_SERVICE_URL=http://inventory-service:8080`
- `INVENTORY_SERVICE_TIMEOUT=5000`
- `INVENTORY_SERVICE_RETRY_ATTEMPTS=3`
- Actualizar `.env.example` con nuevas variables

##### ‚è≥ T1.6.5: Actualizar tests del Orders Service

- Mockear `InventoryServiceClient` en unit tests
- Crear fixtures para responses del Inventory Service
- Actualizar E2E tests para levantar ambos servicios
- Tests de timeout y retry logic

**Criterios de Aceptaci√≥n:**

- Orders Service no tiene l√≥gica de inventario
- Todas las operaciones de stock se delegan al Inventory Service
- Tests pasan con el cliente HTTP mockeado
- E2E tests funcionan con ambos servicios corriendo

---

### üî¥ Gap 2: Estrategia de Comunicaci√≥n Inter-Servicio Incompleta

**Problema:** Se menciona HTTP y eventos, pero falta decisi√≥n cr√≠tica sobre REST vs gRPC, y la implementaci√≥n de eventos est√° subdesarrollada.

#### Epic 2.5: Sistema de Eventos Distribuidos (RabbitMQ)

**Priority:** CRITICAL | **Effort:** 10h | **Dependencies:** Epic 2.2

##### ‚è≥ T2.5.1: Setup RabbitMQ en docker-compose

- A√±adir servicio RabbitMQ con imagen `rabbitmq:3-management`
- Configurar puerto 5672 (AMQP) y 15672 (Management UI)
- Crear exchanges y queues iniciales
- Configurar persistencia de mensajes
- Health check de RabbitMQ

##### ‚è≥ T2.5.2: Definir eventos de inventario

- Crear schemas de eventos:
  - `InventoryReserved`: cuando se crea una reserva
  - `InventoryConfirmed`: cuando se confirma y decrementa stock
  - `InventoryReleased`: cuando se cancela una reserva
  - `StockDepleted`: cuando un producto llega a quantity = 0
  - `StockReplenished`: cuando se a√±ade stock a un producto
- Documentar estructura de cada evento con ejemplos

##### ‚è≥ T2.5.3: Implementar Publisher en Inventory Service (Go)

- Usar librer√≠a `github.com/rabbitmq/amqp091-go`
- Crear m√≥dulo de eventos con m√©todo `Publish(event Event)`
- Publicar al exchange `inventory.events`
- Garantizar at-least-once delivery
- Manejo de errores de publicaci√≥n

##### ‚è≥ T2.5.4: Implementar Consumer en Orders Service (NestJS)

- Crear m√≥dulo RabbitMQ consumer (adem√°s de Bull existente)
- Suscribirse a eventos de inventario desde queue espec√≠fica
- Procesar eventos y actualizar estado de √≥rdenes
- Idempotencia en procesamiento (evitar duplicados)

##### ‚è≥ T2.5.5: Crear ADR-027: Estrategia de Comunicaci√≥n

- Documentar decisi√≥n: REST para sync, RabbitMQ para async
- Alternativas consideradas: gRPC, Apache Kafka
- Pros y contras de cada opci√≥n
- Justificaci√≥n de la decisi√≥n tomada

**Criterios de Aceptaci√≥n:**

- RabbitMQ corriendo en docker-compose
- Inventory publica eventos correctamente
- Orders consume y procesa eventos
- ADR documentado

---

### üî¥ Gap 3: Gesti√≥n de Transacciones Distribuidas (Saga Compensation)

**Problema:** Se menciona Saga Pattern, pero no hay tareas espec√≠ficas para compensaciones complejas cuando Inventory Service falla.

#### Epic 3.3: Compensaci√≥n Distribuida y Manejo de Fallos

**Priority:** CRITICAL | **Effort:** 8h | **Dependencies:** Epic 3.1, 3.2

##### ‚è≥ T3.3.1: Implementar patr√≥n Two-Phase Commit simplificado

- Phase 1: Reserve - reserva temporal en Inventory Service
- Phase 2: Confirm o Release seg√∫n resultado de pago
- Timeout de 15 minutos para confirmaci√≥n
- Auto-release si no se confirma a tiempo

##### ‚è≥ T3.3.2: Manejar fallos de red entre servicios

- Si Inventory no responde, retry 3 veces con backoff
- Si falla definitivamente, marcar orden como FAILED
- Registrar error detallado en logs
- Enviar notificaci√≥n al cliente sobre fallo

##### ‚è≥ T3.3.3: Implementar Dead Letter Queue para eventos fallidos

- Si Orders no puede procesar evento de inventario, enviar a DLQ
- Crear endpoint administrativo para revisar DLQ
- Dashboard para monitorear eventos fallidos
- Manual retry de eventos desde DLQ

##### ‚è≥ T3.3.4: Crear tests de "Chaos Engineering" b√°sicos

- Test: Simular Inventory Service completamente ca√≠do
- Test: Simular latencia extrema de red (>2 segundos)
- Test: Simular respuestas malformadas del Inventory
- Verificar que Orders Service no se bloquea ni crashea

**Criterios de Aceptaci√≥n:**

- Compensaciones funcionan correctamente en todos los escenarios
- No hay √≥rdenes en estado inconsistente
- DLQ captura eventos fallidos
- Tests de caos pasan exitosamente

---

### üî¥ Gap 4: Cache Distribuida con Redis (Incompleto)

**Problema:** Se menciona cach√© en Inventory, pero falta estrategia de invalidaci√≥n y sincronizaci√≥n entre servicios.

#### Epic 2.6: Sistema de Cach√© Distribuida

**Priority:** HIGH | **Effort:** 6h | **Dependencies:** Epic 2.3

##### ‚è≥ T2.6.1: Implementar Cache-Aside Pattern en Inventory

- GET `/inventory/:productId` ‚Üí leer de Redis primero
- Si cache miss, leer de PostgreSQL y escribir a Redis
- TTL configurable: 5 minutos por defecto
- Serializaci√≥n eficiente de datos

##### ‚è≥ T2.6.2: Invalidaci√≥n de cach√© al actualizar stock

- Al reservar stock, invalidar key en Redis
- Al confirmar reserva, invalidar key
- Al liberar reserva, invalidar key
- Usar patr√≥n "write-through" para consistencia

##### ‚è≥ T2.6.3: Cach√© de agregaciones

- Cachear query "low stock products" (pesada, muchos JOINs)
- Cachear "most reserved products" para analytics
- Cachear estad√≠sticas globales de inventario
- TTL m√°s largo para agregaciones (15 min)

##### ‚è≥ T2.6.4: Configurar Redis Cluster (opcional para portfolio avanzado)

- Setup master-replica para alta disponibilidad
- Configurar Redis Sentinel para failover autom√°tico
- Documentar arquitectura de Redis distribuido
- Health checks de Redis cluster

**Criterios de Aceptaci√≥n:**

- Cache-aside funciona correctamente
- Invalidaci√≥n es inmediata al actualizar datos
- Latencia de queries con cache <50ms P95
- Tests de invalidaci√≥n pasan

---

### üî¥ Gap 5: Observabilidad y Debugging Distribuido

**Problema:** Se tienen m√©tricas y logs, pero falta distributed tracing, cr√≠tico para debugging de microservicios.

#### Epic 6.4: Distributed Tracing

**Priority:** CRITICAL | **Effort:** 8h | **Dependencies:** Epic 6.1, 6.2

##### ‚è≥ T6.4.1: Implementar OpenTelemetry en ambos servicios

- Instalar SDK de OpenTelemetry para Go
- Instalar SDK de OpenTelemetry para Node.js
- Instrumentar endpoints HTTP autom√°ticamente
- Instrumentar llamadas a base de datos
- Propagar trace context en headers HTTP

##### ‚è≥ T6.4.2: Setup Jaeger para visualizaci√≥n de traces

- A√±adir Jaeger all-in-one a docker-compose
- Configurar exporters en ambos servicios hacia Jaeger
- UI disponible en `http://localhost:16686`
- Configurar sampling (100% en dev, 10% en prod)

##### ‚è≥ T6.4.3: Crear Correlation IDs unificados

- Generar UUID √∫nico en API Gateway para cada request
- Propagar en header `X-Correlation-ID`
- Incluir correlation ID en todos los logs estructurados
- Incluir en respuestas de error para debugging

##### ‚è≥ T6.4.4: Dashboards de latencia cross-service

- Crear Grafana dashboard con m√©tricas de tracing
- Visualizar latencia P50/P95/P99 por servicio
- Visualizar tiempo total de procesamiento de una orden
- Visualizar tasa de errores distribuida por servicio

**Criterios de Aceptaci√≥n:**

- Traces visibles en Jaeger UI
- Correlation IDs presentes en todos los logs
- Dashboard de Grafana funcionando
- Latencias medibles end-to-end

---

### üî¥ Gap 6: API Gateway - Funcionalidad Incompleta

**Problema:** Se definen rutas b√°sicas, pero faltan features cr√≠ticos de un API Gateway real de nivel empresarial.

#### Epic 4.2: Funcionalidades Avanzadas del API Gateway

**Priority:** HIGH | **Effort:** 10h | **Dependencies:** Epic 4.1

##### ‚è≥ T4.2.1: Implementar Rate Limiting global

- Limitar a 100 requests/minuto por IP
- Usar Redis para contadores distribuidos
- Retornar 429 Too Many Requests cuando se excede
- Headers informativos: `X-RateLimit-Limit`, `X-RateLimit-Remaining`

##### ‚è≥ T4.2.2: Implementar Request/Response Logging

- Log de todos los requests entrantes con correlation ID
- Log de response times para m√©tricas
- Log de errores 4xx y 5xx con detalles
- Integraci√≥n con Winston para logging estructurado

##### ‚è≥ T4.2.3: Implementar Circuit Breaker a nivel Gateway

- Monitorear error rate de cada servicio downstream
- Si un servicio tiene >50% error rate, abrir circuit
- Retornar 503 Service Unavailable inmediatamente
- Auto-cierre despu√©s de timeout configurable

##### ‚è≥ T4.2.4: Configurar CORS policies

- Permitir origenes espec√≠ficos (whitelist)
- Configurar m√©todos HTTP permitidos
- Configurar headers permitidos y expuestos
- Preflight requests manejados correctamente

##### ‚è≥ T4.2.5: Implementar Load Balancing b√°sico

- Detectar m√∫ltiples instancias del mismo servicio
- Algoritmo round-robin simple para distribuci√≥n
- Health checks para remover instancias no saludables
- Sticky sessions si es necesario

##### ‚è≥ T4.2.6: Crear ADR-028: Selecci√≥n de API Gateway

- Decisi√≥n final: Express custom vs Kong vs Traefik
- Evaluaci√≥n de features necesarios vs disponibles
- Consideraciones de complejidad para portfolio
- Justificaci√≥n t√©cnica de la elecci√≥n

**Criterios de Aceptaci√≥n:**

- Rate limiting funcional
- Circuit breaker previene cascading failures
- CORS configurado correctamente
- ADR documentado con decisi√≥n clara

---

### üî¥ Gap 7: Testing de Concurrencia (Feature Estrella)

**Problema:** Se menciona "test de concurrencia" vagamente, pero es el feature m√°s importante del proyecto.

#### Epic 5.3: Suite Completa de Tests de Concurrencia

**Priority:** CRITICAL | **Effort:** 12h | **Dependencies:** Epic 2.2, 2.3

##### ‚è≥ T5.3.1: Test: Race condition al reservar √∫ltimo √≠tem

- 100 goroutines intentan reservar simult√°neamente quantity=1
- Solo 1 debe tener √©xito (200 OK)
- 99 deben recibir 409 Conflict (insufficient stock)
- Verificar en base de datos: `reserved = 1`, no m√°s
- Usar testcontainers para PostgreSQL real

##### ‚è≥ T5.3.2: Test: Locking optimista con versiones

- Thread 1 lee item (version=1)
- Thread 2 lee item (version=1)
- Thread 1 actualiza con √©xito (version=2)
- Thread 2 intenta actualizar con version=1 ‚Üí FAIL 409
- Verificar mensaje de error apropiado

##### ‚è≥ T5.3.3: Test: Reservas expiradas liberan stock correctamente

- Crear 10 reservas con expiraci√≥n en 1 segundo
- Esperar 2 segundos
- Ejecutar cronjob de expiraci√≥n manualmente
- Verificar que stock disponible aument√≥ exactamente en 10 unidades

##### ‚è≥ T5.3.4: Test de carga con K6

- Crear script de K6 para simular 1000 usuarios concurrentes
- Cada usuario intenta crear una orden simult√°neamente
- Medir latencia P95 (target: <200ms)
- Verificar 0 race conditions en stock
- Generar reporte HTML con resultados

##### ‚è≥ T5.3.5: Test: Deadlock prevention

- Orden 1 intenta reservar producto A y B
- Orden 2 intenta reservar producto B y A (orden inverso)
- Ambas √≥rdenes se ejecutan simult√°neamente
- Verificar que no hay deadlock (timeout o success en ambas)
- Medir tiempo de resoluci√≥n

**Criterios de Aceptaci√≥n:**

- 100% de tests de concurrencia pasan
- 0 race conditions detectadas
- Locking optimista funciona correctamente
- Reporte de K6 demuestra performance bajo carga

---

### üü° Gap 8: Seguridad (Casi Ausente)

**Problema:** Solo se menciona "autenticaci√≥n centralizada" vagamente, pero la seguridad necesita m√°s atenci√≥n.

#### Epic 4.3: Seguridad del Ecosistema

**Priority:** HIGH | **Effort:** 6h | **Dependencies:** Epic 4.1

##### ‚è≥ T4.3.1: Implementar Service-to-Service Authentication

- API keys compartidas entre servicios internos
- O JWT espec√≠fico para comunicaci√≥n inter-servicio
- Inventory Service valida requests de Orders Service
- Prevenir acceso directo desde internet (solo via Gateway)

##### ‚è≥ T4.3.2: Implementar Input Validation en Inventory

- Validar formato de UUIDs
- Validar rangos de quantity (min: 1, max: 1000)
- Sanitizar todos los inputs para prevenir SQL injection
- Retornar 400 Bad Request con errores descriptivos

##### ‚è≥ T4.3.3: Rate Limiting por servicio

- Inventory Service: 200 req/min por cliente
- Orders Service: 100 req/min por usuario
- Rate limiting diferenciado por endpoint

##### ‚è≥ T4.3.4: Secrets Management

- Usar Docker secrets o variables de entorno protegidas
- NO commitear credenciales en c√≥digo fuente
- Documentar proceso de rotaci√≥n de passwords de BD
- Usar secretos diferentes en dev, test y prod

**Criterios de Aceptaci√≥n:**

- Servicios no son accesibles sin autenticaci√≥n
- Input validation previene ataques comunes
- Secrets management implementado
- Documentaci√≥n de seguridad completa

---

### üü° Gap 9: Data Management y Migraciones

**Problema:** Se definen migraciones b√°sicamente, pero falta estrategia de datos de prueba y sincronizaci√≥n.

#### Epic 2.7: Gesti√≥n de Datos y Migraciones

**Priority:** MEDIUM | **Effort:** 6h | **Dependencies:** Epic 2.3

##### ‚è≥ T2.7.1: Crear migraciones iniciales para Inventory

- `001_create_inventory_items.sql`
- `002_create_reservations.sql`
- `003_add_indexes.sql`
- `004_add_constraints.sql`

##### ‚è≥ T2.7.2: Crear seeders para desarrollo

- Seed 100 productos en Inventory Service
- Sincronizar UUIDs con productos del Orders Service
- Crear script reutilizable para recrear datos

##### ‚è≥ T2.7.3: Script de sincronizaci√≥n de datos

- Script que copia `product_ids` de Orders a Inventory
- √ötil al migrar de monolito a microservicios
- Maneja productos nuevos y existentes
- Logging detallado de operaciones

##### ‚è≥ T2.7.4: Estrategia de rollback de migraciones

- Definir proceso manual de rollback
- Crear migraciones "down" para cada migraci√≥n "up"
- Documentar escenarios de rollback
- Tests de migraciones up y down

**Criterios de Aceptaci√≥n:**

- Migraciones se ejecutan sin errores
- Seeds crean datos consistentes
- Script de sincronizaci√≥n funciona
- Rollbacks documentados

---

### üü° Gap 10: Performance y Optimizaci√≥n

**Problema:** No hay √©pica dedicada a optimizaci√≥n, cr√≠tico para demostrar expertise t√©cnico.

#### Epic 5.4: Optimizaci√≥n y Performance

**Priority:** MEDIUM | **Effort:** 8h | **Dependencies:** Epic 5.1

##### ‚è≥ T5.4.1: Benchmarking de endpoints cr√≠ticos

- Benchmark: GET `/inventory/:id` (target: <50ms P95)
- Benchmark: POST `/inventory/reserve` (target: <100ms P95)
- Usar herramienta como Apache Bench o wrk
- Generar reportes de performance

##### ‚è≥ T5.4.2: Optimizaci√≥n de queries SQL

- A√±adir √≠ndices compuestos donde sea necesario
- Analizar EXPLAIN de queries lentas
- Optimizar JOINs innecesarios
- Documentar decisiones de indexaci√≥n

##### ‚è≥ T5.4.3: Connection Pooling optimizado

- PostgreSQL: max 20 conexiones por servicio
- Redis: max 10 conexiones
- Timeout de conexi√≥n: 5 segundos
- Monitoring de pool saturation

##### ‚è≥ T5.4.4: Compresi√≥n de responses

- Implementar gzip para responses >1KB
- Medir impacto en latencia
- Balance entre CPU y bandwidth
- Configurar en API Gateway

**Criterios de Aceptaci√≥n:**

- Benchmarks documentados
- Optimizaciones implementadas mejoran performance
- Connection pools configurados √≥ptimamente
- Compresi√≥n reduce payload size

---

### üü° Gap 11: Documentaci√≥n T√©cnica de Arquitectura

**Problema:** Se tienen ADRs gen√©ricos, pero faltan diagramas detallados y documentaci√≥n de flujos.

#### Epic 7.3: Documentaci√≥n de Arquitectura

**Priority:** MEDIUM | **Effort:** 6h | **Dependencies:** Todas las fases anteriores

##### ‚è≥ T7.3.1: Diagrama C4 Model - Nivel 1 (Context)

- Identificar actores externos (Cliente, Admin)
- Mostrar sistema completo como caja negra
- Incluir sistemas externos (Email Service, Payment Gateway)
- Documentar interacciones de alto nivel

##### ‚è≥ T7.3.2: Diagrama C4 Model - Nivel 2 (Containers)

- Descomponer en: API Gateway, Orders Service, Inventory Service
- Incluir: Bases de datos, Redis, RabbitMQ
- Mostrar protocolos de comunicaci√≥n (HTTP, AMQP)
- Documentar puertos y endpoints

##### ‚è≥ T7.3.3: Diagrama de Secuencia: Happy Path de Orden

- Flujo completo: Cliente ‚Üí Gateway ‚Üí Orders ‚Üí Inventory ‚Üí Payment
- Incluir tiempos aproximados de cada paso
- Mostrar comunicaci√≥n s√≠ncrona y as√≠ncrona
- Documentar estados de la orden en cada paso

##### ‚è≥ T7.3.4: Diagrama de Secuencia: Compensaci√≥n en Fallo

- Flujo cuando falla el procesamiento de pago
- Orders llama a Inventory para liberar reserva
- Actualizaci√≥n de estado de orden
- Notificaci√≥n al cliente

##### ‚è≥ T7.3.5: Documento: Estrategia de Deployment

- Orden correcto de inicio de servicios
- Healthchecks y readiness probes configurados
- Estrategia de rollback en caso de fallo
- Checklist de deployment

**Criterios de Aceptaci√≥n:**

- Todos los diagramas creados en Mermaid
- Documentaci√≥n incluida en repositorio
- Diagramas referenciados en README principal
- Estrategia de deployment documentada

---

## üìä Resumen de Gaps por Criticidad

| Gap | Categor√≠a                   | Criticidad | Tareas | Effort |
| --- | --------------------------- | ---------- | ------ | ------ |
| 1   | Refactoring Orders          | üî¥ CR√çTICO | 5      | 12h    |
| 2   | Comunicaci√≥n Inter-Servicio | üî¥ CR√çTICO | 5      | 10h    |
| 3   | Saga Compensation           | üî¥ CR√çTICO | 4      | 8h     |
| 4   | Cach√© Distribuida           | üü° MEDIO   | 4      | 6h     |
| 5   | Distributed Tracing         | üî¥ CR√çTICO | 4      | 8h     |
| 6   | API Gateway Avanzado        | üü° MEDIO   | 6      | 10h    |
| 7   | Tests de Concurrencia       | üî¥ CR√çTICO | 5      | 12h    |
| 8   | Seguridad                   | üî¥ CR√çTICO | 4      | 6h     |
| 9   | Data Management             | üü° MEDIO   | 4      | 6h     |
| 10  | Performance                 | üü° MEDIO   | 4      | 8h     |
| 11  | Docs Arquitectura           | üü° MEDIO   | 5      | 6h     |

**Totales:**

- **Tareas adicionales necesarias:** 50 tareas
- **Effort total:** 92 horas (~2.5 semanas adicionales)
- **Gaps cr√≠ticos (üî¥):** 6 √©picas / 31 tareas
- **Gaps medios (üü°):** 5 √©picas / 19 tareas

---

## üéØ Recomendaciones de Acci√≥n

### 1Ô∏è‚É£ Prioriza los Gaps Cr√≠ticos Primero

Los siguientes gaps son **no negociables** para un portfolio profesional de microservicios:

- Gap 1: Refactoring de Orders Service
- Gap 2: Sistema de eventos con RabbitMQ
- Gap 3: Compensaci√≥n distribuida
- Gap 5: Distributed tracing
- Gap 7: Tests de concurrencia exhaustivos
- Gap 8: Seguridad b√°sica

**Acci√≥n:** Integra estos 6 gaps antes de comenzar la Fase 2.

### 2Ô∏è‚É£ Reorganiza las Fases del Proyecto

**Estructura Sugerida:**

```
FASE 0: Technical Spikes (3 d√≠as)
  ‚îî‚îÄ‚îÄ Investigaci√≥n de tecnolog√≠as clave

FASE 1: Setup + Refactoring (Semana 1-2)
  ‚îú‚îÄ‚îÄ √âpicas existentes 1.1-1.5 ‚úÖ
  ‚îú‚îÄ‚îÄ Epic 1.6: Refactoring Orders Service üî¥
  ‚îî‚îÄ‚îÄ Epic 1.7: Setup RabbitMQ üî¥

FASE 2: Inventory Core + Comunicaci√≥n (Semana 3-4)
  ‚îú‚îÄ‚îÄ √âpicas existentes 2.1-2.4 ‚úÖ
  ‚îú‚îÄ‚îÄ Epic 2.5: Sistema de Eventos üî¥
  ‚îú‚îÄ‚îÄ Epic 2.6: Cach√© Distribuida üü°
  ‚îî‚îÄ‚îÄ Epic 2.7: Data Management üü°

FASE 3: Integraci√≥n Cross-Service (Semana 5)
  ‚îú‚îÄ‚îÄ √âpicas existentes 3.1-3.2 ‚úÖ
  ‚îî‚îÄ‚îÄ Epic 3.3: Compensaci√≥n Distribuida üî¥

FASE 4: API Gateway (Semana 6)
  ‚îú‚îÄ‚îÄ Epic existente 4.1 ‚úÖ
  ‚îú‚îÄ‚îÄ Epic 4.2: Features Avanzados üü°
  ‚îî‚îÄ‚îÄ Epic 4.3: Seguridad üî¥

FASE 5: Testing Exhaustivo (Semana 7-8)
  ‚îú‚îÄ‚îÄ √âpicas existentes 5.1-5.2 ‚úÖ
  ‚îú‚îÄ‚îÄ Epic 5.3: Tests de Concurrencia üî¥
  ‚îî‚îÄ‚îÄ Epic 5.4: Performance üü°

FASE 6: Observabilidad (Semana 9)
  ‚îú‚îÄ‚îÄ √âpicas existentes 6.1-6.3 ‚úÖ
  ‚îî‚îÄ‚îÄ Epic 6.4: Distributed Tracing üî¥

FASE 7: Documentaci√≥n + Deploy (Semana 10)
  ‚îú‚îÄ‚îÄ √âpicas existentes 7.1-7.2 ‚úÖ
  ‚îî‚îÄ‚îÄ Epic 7.3: Docs de Arquitectura üü°
```

### 3Ô∏è‚É£ A√±ade "Definition of Done" a Cada Epic

**Ejemplo para Epic 2.1:**

```markdown
## Epic 2.1: Domain Layer - Entidades y L√≥gica de Negocio

‚úÖ Definition of Done:

- [ ] Todas las entidades tienen tests unitarios con coverage >80%
- [ ] Value Objects validados con casos edge (negativos, l√≠mites)
- [ ] Errores de dominio documentados en c√≥digo con ejemplos
- [ ] Interfaces de repositorios definidas y documentadas
- [ ] Code review completado por segundo desarrollador
- [ ] Documentaci√≥n t√©cnica actualizada
```

El Epic 0 deber√≠a enfocarse en decisiones pendientes reales, no en cosas ya definidas:
Epic 0.1: Technical Spikes (CORREGIDO)
‚è≥ Spike 0.1.1: Selecci√≥n de API Gateway para Portfolio
Contexto: Necesitamos un gateway que enrute a Orders (NestJS/REST) e Inventory (Go/REST).
Opciones a evaluar:

Express custom con http-proxy-middleware

Pro: Control total, f√°cil de entender para recruiters
Pro: Mismo stack que Orders (Node.js)
Contra: Ten√©s que implementar todo (rate limiting, circuit breaker, etc.)

Kong (Open Source)

Pro: Nivel empresarial, muchas features out-of-the-box
Contra: Complejidad de setup puede opacar el proyecto
Contra: Puede parecer "overkill" para un portfolio

Traefik

Pro: Configuraci√≥n simple con docker labels
Pro: Auto-descubrimiento de servicios
Contra: Menos control granular

Decisi√≥n esperada: Express custom (recomendado para portfolio - m√°xima transparencia)

‚è≥ Spike 0.1.2: Testcontainers en Go - Viabilidad para CI/CD
Contexto: Inventory Service (Go) necesita tests de integraci√≥n con PostgreSQL real.
A investigar:

¬øTestcontainers funciona bien en GitHub Actions?
¬øTiempo de setup es aceptable? (<2 min ideal)
¬øAlternativas como sqlmock son suficientes para el portfolio?

Entregable: PoC de test con testcontainers + decisi√≥n documentada

‚è≥ Spike 0.1.3: Estrategia de Comunicaci√≥n S√≠ncrona
Contexto: Orders (NestJS) necesita llamar a Inventory (Go/Gin).
Ya decidido: REST (ambos servicios son RESTful)
A definir:

¬øCliente HTTP nativo de NestJS (@nestjs/axios) o librer√≠a custom?
¬øTimeout strategy? (5s, 10s?)
¬øRetry autom√°tico o manual?
¬øCircuit breaker a nivel de cliente o gateway?

Entregable: ADR con decisiones de implementaci√≥n

‚è≥ Spike 0.1.4: RabbitMQ vs Redis Pub/Sub para eventos as√≠ncronos
Contexto: Inventory necesita publicar eventos (InventoryReserved, etc.) que Orders consume.
Opciones:

RabbitMQ (nuevo en el stack)

Pro: Garant√≠as de entrega m√°s fuertes
Pro: Queues persistentes
Contra: A√±ade complejidad al docker-compose

Redis Pub/Sub (ya ten√©s Redis del Proyecto 2)

Pro: Infraestructura existente
Pro: M√°s simple
Contra: No persiste mensajes (at-most-once delivery)

Recomendaci√≥n para portfolio: RabbitMQ (demuestra m√°s conocimiento de message brokers)
